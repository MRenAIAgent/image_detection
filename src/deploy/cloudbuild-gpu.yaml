# Cloud Build configuration for GPU-enabled Image Recognition API deployment
steps:
  # Step 1: Build the FastAPI client Docker image
  - name: 'gcr.io/cloud-builders/docker'
    args: 
      - 'build'
      - '-t'
      - 'gcr.io/$PROJECT_ID/image-recognition-api:$COMMIT_SHA'
      - '-t'
      - 'gcr.io/$PROJECT_ID/image-recognition-api:latest'
      - '-f'
      - 'Dockerfile.client'
      - '--target'
      - 'production'
      - '.'
    
  # Step 2: Push the image to Container Registry
  - name: 'gcr.io/cloud-builders/docker'
    args:
      - 'push'
      - 'gcr.io/$PROJECT_ID/image-recognition-api:$COMMIT_SHA'
      
  - name: 'gcr.io/cloud-builders/docker'
    args:
      - 'push'
      - 'gcr.io/$PROJECT_ID/image-recognition-api:latest'
      
  # Step 3: Setup model (download and convert YOLOv8n with TensorRT optimization)
  - name: 'python:3.11-slim'
    entrypoint: 'bash'
    args:
      - '-c'
      - |
        apt-get update && apt-get install -y curl libgl1-mesa-glx libglib2.0-0
        pip install ultralytics torch onnx opencv-python pillow numpy tensorrt
        python setup_model.py --model-dir models --enable-tensorrt
        ls -la models/model_repository/yolov8n/1/
    
  # Step 4: Create Cloud Storage bucket for models (if not exists)
  - name: 'gcr.io/cloud-builders/gsutil'
    args:
      - 'mb'
      - '-p'
      - '$PROJECT_ID'
      - 'gs://$PROJECT_ID-image-recognition-models-gpu'
    allowFailure: true
    
  # Step 5: Upload model repository to Cloud Storage
  - name: 'gcr.io/cloud-builders/gsutil'
    args:
      - '-m'
      - 'rsync'
      - '-r'
      - '-d'
      - 'models/model_repository'
      - 'gs://$PROJECT_ID-image-recognition-models-gpu/model_repository'
    
  # Step 6: Deploy Triton server to GKE with GPU support
  - name: 'gcr.io/cloud-builders/kubectl'
    args:
      - 'apply'
      - '-f'
      - '-'
    env:
      - 'CLOUDSDK_COMPUTE_ZONE=$_ZONE'
      - 'CLOUDSDK_CONTAINER_CLUSTER=$_CLUSTER'
    stdin: |
      apiVersion: apps/v1
      kind: Deployment
      metadata:
        name: triton-server-gpu-$_HASH
        labels:
          app: triton-server-gpu
          version: $_HASH
      spec:
        replicas: 2
        selector:
          matchLabels:
            app: triton-server-gpu
            version: $_HASH
        template:
          metadata:
            labels:
              app: triton-server-gpu
              version: $_HASH
          spec:
            nodeSelector:
              cloud.google.com/gke-accelerator: nvidia-tesla-t4
            containers:
            - name: triton-server
              image: nvcr.io/nvidia/tritonserver:23.10-py3
              command: ["tritonserver"]
              args:
                - --model-repository=gs://$PROJECT_ID-image-recognition-models-gpu/model_repository
                - --allow-http=true
                - --allow-grpc=true
                - --allow-metrics=true
                - --strict-model-config=false
                - --backend-config=tensorrt,default-max-batch-size=32
              ports:
              - containerPort: 8000
                name: http
              - containerPort: 8001
                name: grpc
              - containerPort: 8002
                name: metrics
              resources:
                requests:
                  nvidia.com/gpu: 1
                  memory: "8Gi"
                  cpu: "4"
                limits:
                  nvidia.com/gpu: 1
                  memory: "16Gi"
                  cpu: "8"
              env:
              - name: NVIDIA_VISIBLE_DEVICES
                value: "all"
              - name: CUDA_VISIBLE_DEVICES
                value: "0"
      ---
      apiVersion: v1
      kind: Service
      metadata:
        name: triton-server-gpu-service-$_HASH
        labels:
          app: triton-server-gpu
      spec:
        type: LoadBalancer
        ports:
        - port: 8000
          targetPort: 8000
          name: http
        - port: 8001
          targetPort: 8001
          name: grpc
        - port: 8002
          targetPort: 8002
          name: metrics
        selector:
          app: triton-server-gpu
          version: $_HASH
      
  # Step 7: Get Triton service URL
  - name: 'gcr.io/cloud-builders/kubectl'
    entrypoint: 'bash'
    args:
      - '-c'
      - |
        # Wait for LoadBalancer IP
        for i in {1..20}; do
          TRITON_IP=$(kubectl get service triton-server-gpu-service-$_HASH -o jsonpath='{.status.loadBalancer.ingress[0].ip}')
          if [[ -n "$TRITON_IP" ]]; then
            echo "Triton GPU service IP: $TRITON_IP"
            echo "$TRITON_IP:8001" > /workspace/triton_gpu_url.txt
            break
          fi
          sleep 30
        done
    env:
      - 'CLOUDSDK_COMPUTE_ZONE=$_ZONE'
      - 'CLOUDSDK_CONTAINER_CLUSTER=$_CLUSTER'
      
  # Step 8: Deploy FastAPI client to Cloud Run
  - name: 'gcr.io/cloud-builders/gcloud'
    entrypoint: 'bash'
    args:
      - '-c'
      - |
        TRITON_GRPC_URL=$(cat /workspace/triton_gpu_url.txt || echo "triton-server-gpu:8001")
        echo "Deploying API with GPU Triton URL: $TRITON_GRPC_URL"
        
        gcloud run deploy image-recognition-api-gpu-$_HASH \
          --image=gcr.io/$PROJECT_ID/image-recognition-api:$COMMIT_SHA \
          --platform=managed \
          --region=$_REGION \
          --allow-unauthenticated \
          --port=8080 \
          --memory=4Gi \
          --cpu=2 \
          --min-instances=2 \
          --max-instances=50 \
          --timeout=300 \
          --concurrency=200 \
          --set-env-vars="TRITON_URL=$TRITON_GRPC_URL,LOG_LEVEL=INFO,MAX_BATCH_SIZE=32,CONFIDENCE_THRESHOLD=0.5,BATCH_TIMEOUT=0.05,MAX_QUEUE_SIZE=200,ENVIRONMENT=production,ENABLE_GPU=true"
       
  # Step 9: Create Cloud Scheduler job for health checks
  - name: 'gcr.io/cloud-builders/gcloud'
    entrypoint: 'bash'
    args:
      - '-c'
      - |
        # Delete existing job if it exists
        gcloud scheduler jobs delete image-recognition-health-check-gpu-$_HASH --location=$_REGION --quiet || true
        
        # Get API URL
        API_URL=$(gcloud run services describe image-recognition-api-gpu-$_HASH --region=$_REGION --format="value(status.url)")
        
        # Create new health check job
        gcloud scheduler jobs create http image-recognition-health-check-gpu-$_HASH \
          --schedule="*/2 * * * *" \
          --uri="$API_URL/health" \
          --http-method=GET \
          --location=$_REGION \
          --description="Health check for GPU Image Recognition API"
          
  # Step 10: Output deployment information
  - name: 'gcr.io/cloud-builders/gcloud'
    entrypoint: 'bash'
    args:
      - '-c'
      - |
        echo "=== GPU DEPLOYMENT COMPLETED ==="
        API_URL=$(gcloud run services describe image-recognition-api-gpu-$_HASH --region=$_REGION --format="value(status.url)")
        TRITON_URL=$(cat /workspace/triton_gpu_url.txt || echo "Check GKE service")
        
        echo "GPU API URL: $API_URL"
        echo "GPU API Docs: $API_URL/docs"
        echo "GPU Triton URL: http://$TRITON_URL"
        echo "Health Check: $API_URL/health"
        echo "Model Info: $API_URL/model/info"
        
        # Save URLs to build artifacts
        echo "{\"api_url\": \"$API_URL\", \"triton_url\": \"$TRITON_URL\", \"deployment_type\": \"gpu\"}" > /workspace/deployment_urls_gpu.json

# Build configuration
options:
  machineType: 'E2_HIGHCPU_8'
  diskSizeGb: 100
  
# Timeout for the entire build
timeout: '3600s'

# Substitutions
substitutions:
  _REGION: 'us-central1'
  _ZONE: 'us-central1-a'
  _CLUSTER: 'image-recognition-gpu-cluster'
  _HASH: '${COMMIT_SHA:0:7}'
  
# Artifacts
artifacts:
  objects:
    location: 'gs://$PROJECT_ID-build-artifacts'
    paths:
      - '/workspace/deployment_urls_gpu.json' 